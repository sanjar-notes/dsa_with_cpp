# 4. Huffman Encoding
Created Sunday 08 March 2020

## Context
Fixed sized encoding is not efficient. Variable sized encoding (repeated chars are stored in smaller sized codes) would be better.

But how do we define the codes? Huffman's algorithm helps in doing exactly this.
BTW, we do concatenation here, not frequency (number) and code.

Note:
- There are versions of thinking about encoding:
	1. Fixed size + concatenated. ineffecient.
	2. Fixed size code + frequency (not concatenating the same char). equally inefficient as above, asymptotically.
	3. Variable sized + frequency (not possible) ❌
	4. Variable sized + concatenation. ✅
- Escape chars - there can't be escape chars in variable encoding. If we need to have them, then we'd need to define a block size, which is just fixed sized encoding.
- Concatenation instead of frequency - since we can't have escape chars, we can't have numbers, since there's no way to distinguish a char-code and a number.
	- Even if we kept frequency beside char-code in case of fixed size encoding, it would not be effective. What is there are no continuous repeating blocks, and worse, if such distinct windows repeat. It'd be the same as naive (no frequency) fixed sized encoding.
- 

This is a technique for text compression. **Remember that this compression(text) has to be lossless.**

## Huffman algorithm (generating prefix tree)
- Huffman encoding tries to reduce the space requirement for the most ocurring characters. If some rare character(s) need to space greater than 1 bytes, it will be okay, since they are so less.
- Basically we are using different length of bits for each letter. i.e not 8 bits for each letter.
- Note that prefixes for a code cannot be the code for some other character. This happens because, in case of codes with same prefixes, when we reach a letter, the next node **is** a number, **not** a letter and vice-versa. i.e all parents are numbers(sum of frequencies of children).

![](../../../../../../assets/4._Huffman_Encoding-image-1-185a46db.png)
Algorithm(Compression)

1. Count the frequencies and update in a 26 or 256 sized array
2. We need two minimum nodes at each iteration for forming the tree, so use a Min Priority Queue for this. Just pop 2 and push the sum. Keep doing until no letter is left.
3. Make the tree: Make a start node.
4. Save the codes in a hashmap. key is the letter and value is the binary code. We use this to avoid
5. Traverse the message and generate the compressed text.

Note:
- The loop condition is pq.length > 1. i.e. there'll always be one node left, and its fine. This is our root, and its always an aux node (does not contain a value).
  
Send the decomp tree and the compressed text;

Algorithm(Decompression)

1. Make the hashmap for the tree. Keys are compressed texts values are letters.
2. Traverse the compressed text and generate the original text.

This can be done as a project in school.

## Why is Huffman the best?
CLRS assumes without proof that prefix trees allow for optimal compression. Prefix trees means a full-tree that encodes an alphabet so that no two alphabets share a prefix.

What we do prove in CLRS, is that Huffman's algorithm for building a prefix tree is valid. 

There's a trick here, CLRS wrote the algorithm first (without proving), then did the proof. Fine.

Since Huffman is a greedy algorithm, we need to prove that:
1. Greedy property holds, i.e. choosing two minimum from pq retains tree's optimality. *But before proving, we need to define the cost of a tree - its the frequency based sum - frequency and code-size*. We have a tree T which was built with two minimum selected at first, we also consider some larger values in this tree. Then we transform T to T', and then we transform T' to T'' (this one such that two larger values are placed as if they were chosen first). We see that T and T'' are the same in terms of cost. So choosing first two minimum is OK, and doesn't affect generality while maintaining optimality.
2. Optimal substructure exists. Proved using contradiction. We suppose a tree formed by Huffman, T. The PQ has nodes, so we build T'. Now we assume T is not optimal. So a T'' for the same set of nodes must exist that's optimal. We prove that cost of T'' and T is the same. Assumption wrong, meaning T is optimal. TBD.